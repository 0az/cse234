{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ad01a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "# from petastorm.tf_utils import tf_tensors\n",
    "# from petastorm import make_reader, make_batch_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66316935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c4fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f1855f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .parquet(\"./data/dataTraining.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed40686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Humidity: double (nullable = true)\n",
      " |-- Light: double (nullable = true)\n",
      " |-- CO2: double (nullable = true)\n",
      " |-- HumidityRatio: double (nullable = true)\n",
      " |-- Occupancy: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bacaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"date\", F.to_timestamp(F.col(\"date\")))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b6f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"./data/dataTraining.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a470f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, 'hdfs://./data/temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "636d11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "108f6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.orderBy(F.col(\"date\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb5d281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------------+\n",
      "|               date|Temperature|         mean_temp|\n",
      "+-------------------+-----------+------------------+\n",
      "|2015-02-04 17:51:00|      23.18| 20.61908364034639|\n",
      "|2015-02-04 17:51:59|      23.15|20.618769108737492|\n",
      "|2015-02-04 17:53:00|      23.15| 20.61845818490857|\n",
      "|2015-02-04 17:54:00|      23.15|20.618147184685586|\n",
      "|2015-02-04 17:55:00|       23.1|20.617836108040382|\n",
      "|2015-02-04 17:55:59|       23.1|20.617531098960516|\n",
      "|2015-02-04 17:57:00|       23.1|20.617226014912212|\n",
      "|2015-02-04 17:57:59|       23.1| 20.61692085586783|\n",
      "|2015-02-04 17:58:59|       23.1|20.616615621799713|\n",
      "|2015-02-04 18:00:00|     23.075|20.616310312680188|\n",
      "|2015-02-04 18:01:00|     23.075|20.616008002378045|\n",
      "|2015-02-04 18:02:00|       23.1|20.615705617725116|\n",
      "|2015-02-04 18:03:00|       23.1|  20.6154000840414|\n",
      "|2015-02-04 18:04:00|      23.05|20.615094475195647|\n",
      "|2015-02-04 18:04:59|       23.0| 20.61479494197818|\n",
      "|2015-02-04 18:06:00|       23.0|20.614501486631475|\n",
      "|2015-02-04 18:07:00|       23.0|20.614207959067382|\n",
      "|2015-02-04 18:08:00|     22.945|20.613914359259244|\n",
      "|2015-02-04 18:08:59|     22.945| 20.61362745641115|\n",
      "|2015-02-04 18:10:00|      22.89|20.613340482932127|\n",
      "+-------------------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .select(\"date\", \"Temperature\") \\\n",
    "    .withColumn(\"mean_temp\", F.mean(\"Temperature\").over(windowSpec)) \\\n",
    "    .orderBy(\"date\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c44c30e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+----------------+\n",
      "|               date|     Temperature|        nextTemp|\n",
      "+-------------------+----------------+----------------+\n",
      "|2015-02-10 09:33:00|            21.1|            null|\n",
      "|2015-02-10 09:32:00|            21.1|            21.1|\n",
      "|2015-02-10 09:30:59|            21.1|            21.1|\n",
      "|2015-02-10 09:29:59|           21.05|            21.1|\n",
      "|2015-02-10 09:29:00|           21.05|           21.05|\n",
      "|2015-02-10 09:28:00|           21.05|           21.05|\n",
      "|2015-02-10 09:27:00|            21.0|           21.05|\n",
      "|2015-02-10 09:26:00|          21.025|            21.0|\n",
      "|2015-02-10 09:24:59|            21.0|          21.025|\n",
      "|2015-02-10 09:23:59|            21.0|            21.0|\n",
      "|2015-02-10 09:23:00|            21.0|            21.0|\n",
      "|2015-02-10 09:22:00|            21.0|            21.0|\n",
      "|2015-02-10 09:21:00|            21.0|            21.0|\n",
      "|2015-02-10 09:20:00|        20.95875|            21.0|\n",
      "|2015-02-10 09:19:00|         20.9175|        20.95875|\n",
      "|2015-02-10 09:17:59|           20.89|         20.9175|\n",
      "|2015-02-10 09:16:59|         20.9175|           20.89|\n",
      "|2015-02-10 09:16:00|           20.89|         20.9175|\n",
      "|2015-02-10 09:15:00|20.8566666666667|           20.89|\n",
      "|2015-02-10 09:14:00|           20.89|20.8566666666667|\n",
      "+-------------------+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"date\", \"Temperature\").withColumn(\"nextTemp\", F.lag(\"Temperature\", 1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e86c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_spark(spark, \n",
    "                    window_size, \n",
    "                    sampling_factor, \n",
    "                    sample_strategy, \n",
    "                    dataset_path=None,\n",
    "                    id_col='id',\n",
    "                    time_col='time',\n",
    "                    value_col='value',\n",
    "                    label_col='label'):\n",
    "    \"\"\"\n",
    "    Windows data that is already in a spark dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spark: spark session or sql dataframe\n",
    "    dataset_path: str, Default None. path to parquet file\n",
    "    window_size: size of window to use\n",
    "    label_aggregation_strategy: strategy to use for label aggregation. One of 'mean' or 'boolean'  \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    windows: spark.sql.DataFrame containing windows\n",
    "    \n",
    "    \n",
    "    ts_id | ts | label_ts\n",
    "    ------------------\n",
    "    list   list   list\n",
    "    \n",
    "    \n",
    "    Preferred:\n",
    "    id | time      | value ...  | label\n",
    "    --------------------------\n",
    "    int timestamp   float ...   \n",
    "    \"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.dataframe import DataFrame\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql import functions as F\n",
    "    \n",
    "    TIME_COL = 'time'\n",
    "    VALUE_COL = 'value'\n",
    "    LABEL_COL = 'label'\n",
    "    \n",
    "    agg_strategies = {'mean': F.mean, 'boolean': F.max}\n",
    "    \n",
    "    if isinstance(spark, DataFrame):\n",
    "        df = spark\n",
    "    elif isinstance(spark, SparkSession) and dataset_path is not None:\n",
    "        df = spark.read.parquet(dataset_path)\n",
    "    else:\n",
    "        raise TypeError(\"Expected spark context + filepath or spark.sql.DataFrame\")\n",
    "    \n",
    "    # cast timestamp column to timestamp type\n",
    "    df = df.withColumn(time_col, df[time_col].cast('timestamp'))\n",
    "\n",
    "    # apply windowing to values pairs\n",
    "    if id_col in df.columns:\n",
    "        windowSpec = Window.partitionBy(id_col).orderBy(F.col(time_col).desc())\n",
    "        windowSpecLabels = Window.partitionBy(id_col).orderBy(F.col(time_col).desc()).rowsBetween(-window_size, 0)\n",
    "    else:\n",
    "        windowSpec = Window.orderBy(F.col(time_col).desc())\n",
    "        windowSpecLabels = Window.orderBy(F.col(time_col).desc()).rowsBetween(-window_size, 0)\n",
    "    \n",
    "    agg_labels = agg_strategies[sample_strategy](label_col).over(windowSpecLabels)\n",
    "    \n",
    "    windowed_df = df.withColumn(f\"{value_col}_1\", F.lag(value_col, 1).over(windowSpec))\n",
    "    \n",
    "    if window_size > 1:\n",
    "        for i in range(2, window_size + 1):\n",
    "            windowed_df = windowed_df.withColumn(f\"{value_col}_{i}\", F.lag(value_col, i).over(windowSpec))\n",
    "\n",
    "    # remove rows that can't fill a full window and add the labels to the df\n",
    "    return windowed_df.where(F.col(f\"{value_col}_{window_size}\").isNotNull()).withColumn(label_col, agg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1891a250-329b-405d-a2e3-7d5f778cf172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: timestamp, Temperature: double, Occupancy: double, Temperature_1: double, Temperature_2: double, Temperature_3: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepped = prep_data_spark(df.select(\"date\", \"Temperature\", \"Occupancy\"), \n",
    "                3, \n",
    "                1, \n",
    "                \"mean\", \n",
    "                time_col='date', \n",
    "                value_col='Temperature', \n",
    "                label_col='Occupancy')\n",
    "prepped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6dd24",
   "metadata": {},
   "source": [
    "### Cerebro Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f82323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "\n",
    "# datas storage for intermediate data and model artifacts.\n",
    "from cerebro.storage import LocalStore, HDFSStore\n",
    "\n",
    "# Model selection/AutoML methods.\n",
    "from cerebro.tune import GridSearch, RandomSearch, TPESearch\n",
    "\n",
    "# Utility functions for specifying the search space.\n",
    "from cerebro.tune import hp_choice, hp_uniform, hp_quniform, hp_loguniform, hp_qloguniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f5a26d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-26 21:17:37, Running 2 Workers\n"
     ]
    }
   ],
   "source": [
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=2)\n",
    "store = LocalStore(prefix_path='/Users/arunavgupta/Documents/FA21/cse234/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2aac165",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = prepped.drop(F.col(\"date\")).randomSplit([0.8, 0.2])\n",
    "train_df = train_df.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9f5175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(SparkEstimator):\n",
    "    \n",
    "    def _fit(self, dataset):\n",
    "        print(dataset)\n",
    "        assert False\n",
    "\n",
    "\n",
    "def estimator_gen_fn(params):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=4, name='input_layer'))\n",
    "    model.add(tf.keras.layers.Dense(1, input_dim=4))\n",
    "    model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=params['lr'])\n",
    "    loss = 'mse'\n",
    "\n",
    "    estimator = Model(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=['acc'],\n",
    "        batch_size=8)\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "429c47ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'lr': hp_choice([0.01, 0.001, 0.0001])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00e41f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Occupancy: double (nullable = true)\n",
      " |-- Temperature_1: double (nullable = true)\n",
      " |-- Temperature_2: double (nullable = true)\n",
      " |-- Temperature_3: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepped.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2e8d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection = GridSearch(backend, \n",
    "                               store, \n",
    "                               estimator_gen_fn, \n",
    "                               search_space, \n",
    "                               num_epochs=10, \n",
    "                               evaluation_metric='loss',\n",
    "                               label_columns=['Occupancy'], \n",
    "                               feature_columns=['Temperature', 'Temperature_1', 'Temperature_2', 'Temperature_3'], \n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d8a8407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Occupancy: double (nullable = true)\n",
      " |-- Temperature_1: double (nullable = true)\n",
      " |-- Temperature_2: double (nullable = true)\n",
      " |-- Temperature_3: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55d168aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-23 14:30:11, Preparing Data\n",
      "CEREBRO => Time: 2021-11-23 14:30:11, Num Partitions: 2\n",
      "CEREBRO => Time: 2021-11-23 14:30:11, Writing DataFrames\n",
      "CEREBRO => Time: 2021-11-23 14:30:11, Train Data Path: file:///Users/arunavgupta/Documents/FA21/cse234/data/train_data\n",
      "CEREBRO => Time: 2021-11-23 14:30:11, Val Data Path: file:///Users/arunavgupta/Documents/FA21/cse234/data/val_data\n",
      "CEREBRO => Time: 2021-11-23 14:30:12, Train Partitions: 2\n",
      "CEREBRO => Time: 2021-11-23 14:30:12, Val Partitions: 2\n",
      "CEREBRO => Time: 2021-11-23 14:30:13, Train Rows: 4884\n",
      "CEREBRO => Time: 2021-11-23 14:30:13, Val Rows: 1618\n",
      "CEREBRO => Time: 2021-11-23 14:30:13, Initializing Workers\n",
      "CEREBRO => Time: 2021-11-23 14:30:13, Initializing Data Loaders\n",
      "CEREBRO => Time: 2021-11-23 14:30:14, Launching Model Selection Workload\n",
      "CEREBRO => Time: 2021-11-23 14:30:14, Model: model_9_1637706614, lr: 0.01\n",
      "CEREBRO => Time: 2021-11-23 14:30:14, Model: model_10_1637706614, lr: 0.001\n",
      "CEREBRO => Time: 2021-11-23 14:30:14, Model: model_11_1637706614, lr: 0.0001\n",
      "CEREBRO => Time: 2021-11-23 14:30:14, Starting EPOCH Training\n",
      "CEREBRO => Time: 2021-11-23 14:30:14, Scheduling Model: model_9_1637706614, on Worker: 0\n",
      "CEREBRO => Time: 2021-11-23 14:30:14, Scheduled Model: model_9_1637706614, on Worker: 0\n",
      "CEREBRO => Time: 2021-11-23 14:30:14, Scheduling Model: model_11_1637706614, on Worker: 1\n",
      "CEREBRO => Time: 2021-11-23 14:30:14, Scheduled Model: model_11_1637706614, on Worker: 1\n",
      "CEREBRO => Time: 2021-11-23 14:30:16, Terminating Workers\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/tune/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    199\u001b[0m                 datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_on_prepared_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/tune/grid.py\u001b[0m in \u001b[0;36m_fit_on_prepared_data\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_on_prepared_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_fit_on_prepared_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/tune/grid.py\u001b[0m in \u001b[0;36m_fit_on_prepared_data\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    225\u001b[0m         epoch_results = self.backend.train_for_one_epoch(estimators, self.store, self.feature_cols,\n\u001b[0;32m--> 226\u001b[0;31m                                                          self.label_cols)\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0mupdate_model_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/backend/spark/backend.py\u001b[0m in \u001b[0;36mtrain_for_one_epoch\u001b[0;34m(self, models, store, feature_cols, label_cols, is_train)\u001b[0m\n\u001b[1;32m    259\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_epoch_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: in user code:\n\n    /Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/keras/spark/util.py:155 prep  *\n        tuple(\n    /Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/keras/spark/util.py:148 get_col_from_row_fn  *\n        return getattr(row, col)\n\n    AttributeError: 'petastorm_schema_view_view' object has no attribute 'input_layer'\n\nTraceback (most recent call last):\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/backend/spark/service_task.py\", line 199, in bg_execute\n    local_task_index=self.local_task_index)\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/backend/spark/backend.py\", line 527, in train\n    train_data = make_dataset(data_reader, transformation_fn)\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/keras/spark/util.py\", line 75, in fn\n    dataset = dataset.batch(batch_size).map(prep_data_tf_keras, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2517, in map\n    preserve_cardinality=False))\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 4084, in __init__\n    use_legacy_function=use_legacy_function)\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3371, in __init__\n    self._function = wrapper_fn.get_concrete_function()\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2939, in get_concrete_function\n    *args, **kwargs)\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2906, in _get_concrete_function_garbage_collected\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3213, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3075, in _create_graph_function\n    capture_by_value=self._capture_by_value),\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3364, in wrapper_fn\n    ret = _wrapper_helper(*args)\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3299, in _wrapper_helper\n    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\n  File \"/Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 258, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\nAttributeError: in user code:\n\n    /Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/keras/spark/util.py:155 prep  *\n        tuple(\n    /Users/arunavgupta/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/keras/spark/util.py:148 get_col_from_row_fn  *\n        return getattr(row, col)\n\n    AttributeError: 'petastorm_schema_view_view' object has no attribute 'input_layer'\n\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-73d8f6bad97a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/tune/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    206\u001b[0m                     datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_on_prepared_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/backend/spark/backend.py\u001b[0m in \u001b[0;36mteardown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;34m\"\"\"Teardown Spark tasks\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtask_client\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_clients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mtask_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify_workload_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/backend/spark/service_task.py\u001b[0m in \u001b[0;36mnotify_workload_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnotify_workload_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNotifyWorkloadCompleteRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize_data_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore_prefix_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/backend/spark/service_task.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;31m# Since all the addresses were vetted, use the first one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_addresses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maddresses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cerebro/lib/python3.7/site-packages/cerebro_dl-1.0.0-py3.7.egg/cerebro/backend/spark/service_task.py\u001b[0m in \u001b[0;36m_send_one\u001b[0;34m(self, addr, req)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_INET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m                 \u001b[0mrfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0mwfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "model = model_selection.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b708537",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list | grep spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961ed68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf3122eab29cfdd9c1bbf50bd8864ee06abfc7385a6d2e3545b1ae2b457b1a66"
  },
  "kernelspec": {
   "display_name": "cerebro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
