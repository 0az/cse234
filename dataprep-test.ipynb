{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ad01a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from petastorm.tf_utils import tf_tensors\n",
    "from petastorm import make_reader, make_batch_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06a240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(source, window_size, sampling_factor, sample_strategy):\n",
    "    \"\"\"Pulls data from source and preps it into windows given strategy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    source: path to petastorm time series dataset\n",
    "    window_size: size of window to use\n",
    "    sampling_factor: factor to downsample by\n",
    "    sample_strategy: strategy to use for sampling. One of 'mean' or 'boolean'  \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    windows: list of windows\n",
    "    \"\"\"\n",
    "    with make_reader(source) as reader:\n",
    "        dataset = tf_tensors(reader)\n",
    "        X = np.fromiter(map(lambda x: x[0], dataset), dtype=np.float32)\n",
    "        y = np.fromiter(map(lambda x: x[1], dataset), dtype=np.float32)\n",
    "        windowed_ds = keras.utils.timeseries_dataset_from_array(X, y, window_size, sampling_rate = sampling_factor)\n",
    "    \n",
    "    return windowed_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66316935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c4fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f1855f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"./data/dataTraining.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed40686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Humidity: double (nullable = true)\n",
      " |-- Light: double (nullable = true)\n",
      " |-- CO2: double (nullable = true)\n",
      " |-- HumidityRatio: double (nullable = true)\n",
      " |-- Occupancy: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82bacaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------+--------+-----+------+-------------------+---------+\n",
      "|index|               date|Temperature|Humidity|Light|   CO2|      HumidityRatio|Occupancy|\n",
      "+-----+-------------------+-----------+--------+-----+------+-------------------+---------+\n",
      "|    1|2015-02-04 17:51:00|      23.18|  27.272|426.0|721.25|0.00479298817650529|        1|\n",
      "|    2|2015-02-04 17:51:59|      23.15| 27.2675|429.5| 714.0|0.00478344094931065|        1|\n",
      "|    3|2015-02-04 17:53:00|      23.15|  27.245|426.0| 713.5|0.00477946352442199|        1|\n",
      "|    4|2015-02-04 17:54:00|      23.15|    27.2|426.0|708.25|0.00477150882608175|        1|\n",
      "|    5|2015-02-04 17:55:00|       23.1|    27.2|426.0| 704.5|0.00475699293331518|        1|\n",
      "+-----+-------------------+-----------+--------+-----+------+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"date\", F.to_timestamp(F.col(\"date\")))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a5b6f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"./data/dataTraining.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a470f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, 'hdfs://./data/temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636d11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "108f6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.orderBy(F.col(\"date\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5d281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------------+\n",
      "|               date|Temperature|         mean_temp|\n",
      "+-------------------+-----------+------------------+\n",
      "|2015-02-04 17:51:00|      23.18|23.138333333333335|\n",
      "|2015-02-04 17:51:59|      23.15|23.125000000000004|\n",
      "|2015-02-04 17:53:00|      23.15| 23.11666666666667|\n",
      "|2015-02-04 17:54:00|      23.15|23.108333333333334|\n",
      "|2015-02-04 17:55:00|       23.1| 23.09583333333333|\n",
      "|2015-02-04 17:55:59|       23.1|23.091666666666665|\n",
      "|2015-02-04 17:57:00|       23.1|23.091666666666665|\n",
      "|2015-02-04 17:57:59|       23.1| 23.09166666666667|\n",
      "|2015-02-04 17:58:59|       23.1|23.083333333333332|\n",
      "|2015-02-04 18:00:00|     23.075|23.066666666666666|\n",
      "|2015-02-04 18:01:00|     23.075|23.054166666666664|\n",
      "|2015-02-04 18:02:00|       23.1|23.041666666666668|\n",
      "|2015-02-04 18:03:00|       23.1|23.015833333333333|\n",
      "|2015-02-04 18:04:00|      23.05|             22.99|\n",
      "|2015-02-04 18:04:59|       23.0|22.963333333333335|\n",
      "|2015-02-04 18:06:00|       23.0|22.944999999999997|\n",
      "|2015-02-04 18:07:00|       23.0|22.926666666666666|\n",
      "|2015-02-04 18:08:00|     22.945| 22.90833333333333|\n",
      "|2015-02-04 18:08:59|     22.945| 22.89916666666667|\n",
      "|2015-02-04 18:10:00|      22.89|             22.89|\n",
      "+-------------------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .select(\"date\", \"Temperature\") \\\n",
    "    .withColumn(\"mean_temp\", F.mean(\"Temperature\").over(windowSpec)) \\\n",
    "    .orderBy(\"date\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c44c30e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+----------------+\n",
      "|               date|     Temperature|        nextTemp|\n",
      "+-------------------+----------------+----------------+\n",
      "|2015-02-10 09:33:00|            21.1|            null|\n",
      "|2015-02-10 09:32:00|            21.1|            21.1|\n",
      "|2015-02-10 09:30:59|            21.1|            21.1|\n",
      "|2015-02-10 09:29:59|           21.05|            21.1|\n",
      "|2015-02-10 09:29:00|           21.05|           21.05|\n",
      "|2015-02-10 09:28:00|           21.05|           21.05|\n",
      "|2015-02-10 09:27:00|            21.0|           21.05|\n",
      "|2015-02-10 09:26:00|          21.025|            21.0|\n",
      "|2015-02-10 09:24:59|            21.0|          21.025|\n",
      "|2015-02-10 09:23:59|            21.0|            21.0|\n",
      "|2015-02-10 09:23:00|            21.0|            21.0|\n",
      "|2015-02-10 09:22:00|            21.0|            21.0|\n",
      "|2015-02-10 09:21:00|            21.0|            21.0|\n",
      "|2015-02-10 09:20:00|        20.95875|            21.0|\n",
      "|2015-02-10 09:19:00|         20.9175|        20.95875|\n",
      "|2015-02-10 09:17:59|           20.89|         20.9175|\n",
      "|2015-02-10 09:16:59|         20.9175|           20.89|\n",
      "|2015-02-10 09:16:00|           20.89|         20.9175|\n",
      "|2015-02-10 09:15:00|20.8566666666667|           20.89|\n",
      "|2015-02-10 09:14:00|           20.89|20.8566666666667|\n",
      "+-------------------+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"date\", \"Temperature\").withColumn(\"nextTemp\", F.lag(\"Temperature\", 1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e86c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_spark(spark, window_size_data, sampling_factor, sample_strategy, dataset_path=None):\n",
    "    \"\"\"\n",
    "    Windows data that is already in a spark dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spark: spark session or sql dataframe\n",
    "    dataset_path: str, Default None. path to parquet file\n",
    "    window_size: size of window to use\n",
    "    label_aggregation_strategy: strategy to use for label aggregation. One of 'mean' or 'boolean'  \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    windows: spark.sql.DataFrame containing windows\n",
    "    \"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.dataframe import DataFrame\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql import functions as F\n",
    "    \n",
    "    TIME_COL = 'time'\n",
    "    VALUE_COL = 'value'\n",
    "    LABEL_COL = 'label'\n",
    "    \n",
    "    agg_strategies = {'mean': F.mean, 'boolean': F.max}\n",
    "    \n",
    "    if isinstance(spark, DataFrame):\n",
    "        df = spark\n",
    "    elif isinstance(spark, SparkSession) and dataset_path is not None:\n",
    "        df = spark.read.parquet(dataset_path)\n",
    "    else:\n",
    "        raise TypeError(\"Expected spark context + filepath or spark.sql.DataFrame\")\n",
    "    \n",
    "    # cast timestamp column to timestamp type\n",
    "    df = df.withColumn(TIME_COL, df[TIME_COL].cast('timestamp'))\n",
    "\n",
    "    # apply windowing to values pairs\n",
    "    windowSpec = Window.orderBy(F.col(TIME_COL).desc())\n",
    "    windowSpecLabels = Window.orderBy(F.col(TIME_COL).desc()).rowsBetween(-window_size_data, 0)\n",
    "    \n",
    "    agg_labels = agg_strategies[sample_strategy](LABEL_COL).over(windowSpecLabels)\n",
    "    \n",
    "    windowed_df = df.withColumn(f\"{VALUE_COL}_1\", F.lag(VALUE_COL, 1).over(windowSpec))\n",
    "    \n",
    "    if window_size > 1:\n",
    "        for i in range(2, window_size + 1):\n",
    "            windowed_df.withColumn(f\"{VALUE_COL}_{i}\", F.lag(VALUE_COL, i).over(windowSpec))\n",
    "\n",
    "    # remove rows that can't fill a full window and add the labels to the df\n",
    "    return windowed_df.where(F.col(f\"{VALUE_COL}_{window_size}\").isNotNull()).withColumn(LABEL_COL, agg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891a250-329b-405d-a2e3-7d5f778cf172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cerebro",
   "language": "python",
   "name": "cerebro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
